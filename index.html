<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="title" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA">
  <meta name="description" content="We propose KnowCoder-A1, an LLM agent for KBQA that uses outcome-only supervision and multi-stage curriculum reinforcement learning to enhance agentic reasoning, outperforming prior methods.">
  <meta name="keywords" content="KBQA, Knowledge Base Question Answering, Agentic Reasoning, Large Language Models, LLM, Reinforcement Learning, Outcome Supervision, Curriculum Learning">
  <meta name="author" content="Zhuo Chen, Fei Wang, Zixuan Li, Zhao Zhang, Weiwei Ding, Chuanguang Yang, Yongjun Xu, Xiaolong Jin, Jiafeng Guo">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Institute of Computing Technology, Chinese Academy of Sciences">
  <meta property="og:title" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA">
  <meta property="og:description" content="We propose KnowCoder-A1, an LLM agent for KBQA that uses outcome-only supervision and multi-stage curriculum reinforcement learning to enhance agentic reasoning, outperforming prior methods.">
  <meta property="og:url" content="https://<YOUR_GITHUB_USERNAME>.github.io/KnowCoder-A1/">
  <meta property="og:image" content="static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA - Research Preview">
  <meta property="article:published_time" content="2025-10-29T00:00:00.000Z">
  <meta property="article:author" content="Zhuo Chen">
  <meta property="article:section" content="Artificial Intelligence">
  <meta property="article:tag" content="KBQA">
  <meta property="article:tag" content="Agentic Reasoning">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA">
  <meta name="twitter:description" content="We propose KnowCoder-A1, an LLM agent for KBQA that uses outcome-only supervision and multi-stage curriculum reinforcement learning to enhance agentic reasoning, outperforming prior methods.">
  <meta name="twitter:image" content="static/images/social_preview.png">
  <meta name="twitter:image:alt" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA - Research Preview">

  <meta name="citation_title" content="KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA">
  <meta name="citation_author" content="Chen, Zhuo">
  <meta name="citation_author" content="Wang, Fei">
  <meta name="citation_author" content="Li, Zixuan">
  <meta name="citation_author" content="Zhang, Zhao">
  <meta name="citation_author" content="Ding, Weiwei">
  <meta name="citation_author" content="Yang, Chuanguang">
  <meta name="citation_author" content="Xu, Yongjun">
  <meta name="citation_author" content="Jin, Xiaolong">
  <meta name="citation_author" content="Guo, Jiafeng">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="arXiv:2510.25101">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2510.25101.pdf">
  
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>KnowCoder-A1 - Zhuo Chen, et al. | Project Page</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA",
    "description": "We propose KnowCoder-A1, an LLM agent for KBQA that uses outcome-only supervision and multi-stage curriculum reinforcement learning to enhance agentic reasoning, outperforming prior methods.",
    "author": [
      {
        "@type": "Person",
        "name": "Zhuo Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Fei Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Computing Technology, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Zixuan Li",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Computing Technology, Chinese Academy of Sciences"
        }
      }
    ],
    "datePublished": "2025-10-29",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://arxiv.org/abs/2510.25101",
    "image": "static/images/social_preview.png",
    "keywords": ["KBQA", "Knowledge Base Question Answering", "Agentic Reasoning", "Large Language Models", "LLM", "Reinforcement Learning"],
    "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KNOWCODER-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KNOWCODER-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KNOWCODER-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KNOWCODER-Al exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KNOWCODER-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.",
    "citation": "@article{chen2025knowcoder,\n  title={KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA},\n  author={Chen, Zhuo and Wang, Fei and Li, Zixuan and Zhang, Zhao and Ding, Weiwei and Yang, Chuanguang and Xu, Yongjun and Jin, Xiaolong and Guo, Jiafeng},\n  journal={arXiv preprint arXiv:2510.25101},\n  year={2025},\n  eprint={2510.25101},\n  archivePrefix={arXiv},\n  primaryClass={cs.AI}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://arxiv.org/abs/2510.25101"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Knowledge Base Question Answering"
      },
      {
        "@type": "Thing", 
        "name": "Agentic Reasoning"
      }
    ]
  }
  </script>
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Institute of Computing Technology, Chinese Academy of Sciences",
    "url": "http://www.ict.cas.cn/",
    "logo": "static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            
            <h1 class="title is-1 publication-title">KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA</h1>
            <div class="is-size-5 publication-authors">
              
              <span class="author-block">
                <a href="#" target="_blank">Zhuo Chen</a><sup>1,2,3</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Fei Wang</a><sup>1,2*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Zixuan Li</a><sup>1,2*</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Zhao Zhang</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Weiwei Ding</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Chuanguang Yang</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yongjun Xu</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xiaolong Jin</a><sup>1,2,3</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Jiafeng Guo</a><sup>1,2,3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Institute of Computing Technology, Chinese Academy of Sciences</span>
              <span class="author-block"><sup>2</sup>State Key Laboratory of AI Safety</span>
              <span class="author-block"><sup>3</sup>School of Computer Science, University of Chinese Academy of Sciences</span>
              <span class="author-block">arXiv:2510.25101 [cs.AI]</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2510.25101.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.25101" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/YOUR_REPO_HERE" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          </div>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<!-- Opening Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div align=center>
        <img src="./static/images/main.jpeg" alt="main" style="width: 100%;">
        <h2 class="subtitle has-text-centered">
          <div align=justify>
             An overview of the training framework of KNOWCODER-A1 . Stage 1 (left): the SFT-based cold-start process, where high-quality trajectories are curated from strong LLMs to fine-tune an initial agent. Stage 2 (right): the multi-phase Reinforcement Learning curriculum, where the agent is progressively improved through exploration and a dynamic reward strategy.
          </div>
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- Opening Image -->
<!-- Abstract-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability.  </p>
          
          <p>In this paper, we propose <span class="dnerf">Knowcoder-A1</span>, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, <span class="dnerf">Knowcoder-A1</span> trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. </p>
          
          <p>To establish foundational agentic capabilities, <span class="dnerf">Knowcoder-A1</span> first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. </p>
          
          <p>Trained with outcome-only supervision, KNOWCODER-Al exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, <span class="dnerf">Knowcoder-A1</span> achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.. Our code can be found at <a href="https://github.com/agiresearch/ASB">
            <span>https://github.com/agiresearch/ASB</span>.
          </a></p>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Abstract-->
<!-- Introduction-->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
        <div align="center">
          <h2 class="title is-3">ðŸ’¡Introduction</h2>
        </div>

        <br><br>

        <!-- <div class="content has-text-justified">
          <h3 class="subtitle has-text-justified">

            <p>ðŸ’« ASB is a comprehensive benchmarking framework designed to evaluate various adversarial attacks and defenses of LLM-based agents. </p>
            
            <p>ðŸ’« Compared to other benchmarks, ASB's key advantages lie in its inclusion of multiple types of attacks and defense mechanisms across diverse scenarios. </p>
            
            <p>ðŸ’« This not only allows the framework to test agents under more realistic conditions but also to cover a broader spectrum of vulnerabilities and protective strategies. </p>
          </h3>
        </div> -->
        <h3 class="title is-5" > Rollout Scaling: Better Performance with More Rollouts</h3>
          <div>
          <h3 class="title is-5" > More Efficient <em>Rollout Scaling</em> with GRESO </h3>
          <div class="figure">
              <img src="static/images/scaling-compare.png" alt="Scaling" height="400" />
          </div>
          <p>
            In this paper, we aim to design an efficient selective rollout strategy for LLM RL to make rollout scaling more efficient.
            We begin by analyzing the training dynamics of prompts across epochs and observe a strong temporal consistency across different training epochs.
            In particular, prompts that yield zero advantage in one epoch are more likely to do so in future epochs as well.
            This temporal correlation suggests that historical reward dynamics can be leveraged to predict and preemptively skip zero-variance examples before rollout.
            Building on these observations, we propose <strong>GRESO</strong> (<u>GR</u>PO with <u>E</u>fficient <u>S</u>elective R<u>o</u>llout), an online efficient pre-rollout filtering algorithm that reduces rollout cost by selectively skipping prompts predicted to be zero-variance.
            Instead of performing filtering after rollout, GRESO estimates a skipping probability for each prompt based on its reward dynamics during training <u>prior to the rollout stage</u>, significantly reducing prompt selection overhead and making the rollout scaling more efficient.
          </p>

          <p>
            As shown in the above figure, compared to the baseline method (Dynamic Sampling), our approach (GRESO) reduces rollout overhead by up to <b>2x</b> while achieving comparable training performance, improving the efficiency of rollout scaling.
            (We train Qwen2.5-Math-1.5B/7B on the DAPO + MATH dataset and evaluate them on five math reasoning benchmarks: MATH500, AMC, Gaokao, Minerva, and Olympiad Bench.)
          </p>

        </div>

    </div>
  </div>
</section>
<!-- Method-->
<section class="section hero is-light">
<!-- <section class="section hero is-light" style="background-color: #FFFFFC !important;"> -->
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3" style="text-align: center;">
                  <img src="static/images/icon.jpg" style="height: 28px; display: inline; vertical-align: -2px;"/>
                  GRESO: <u>GR</u>PO with <u>E</u>fficient <u>S</u>elective R<u>o</u>llout
                </h2>
                <div class="figure">
                    <img src="static/images/greso-method.jpg" alt="pipeline" height="400" />
                </div>
                <br>
                <div class="content has-text-justified">
                  <h3 class="title is-5" > Observation: Temporal Correlation of Prompts across Epochs </h3>
                  <p>
                      Training data typically exhibits strong temporal correlations across epochs.
                      We hypothesize that zero-variance prompts in GRPO training similarly have such strong correlations in their training dynamics, enabling opportunities for more efficient identification of these prompts prior to the rollout stage.
                      To test this hypothesis, we conduct a study on the temporal correlation of zero-variance prompts in GRPO training.
                      Specifically, we train Qwen2.5-Math-7B with GRPO and measure two probabilities to study the temporal correlation of zero-variance prompts:
                      <b>1)</b> <code>P(Previous|Current)</code>: The probability that a prompt identified as zero-variance in the current epoch was also zero-variance in any previous epoch.
                      <b>2)</b> <code>P(Current|Previous)</code>: The probability that a prompt identified as zero-variance in any previous epoch remains zero-variance in the current epoch.
                      </p>
                      <p>
                      The results shown in the above Figure (a) indicate that zero-variance prompts exhibit strong temporal correlations throughout training.
                      We have two key observations:
                      </p>
                      <ul>
                        <li><em><b>1)</b> Prompts previously identified as zero-variance are likely to remain zero-variance:</em><br>
                        The <code>P(Previous|Current)</code> curve shows that the majority of zero-variance prompts in a given epoch (e.g., over 90%) were also identified as zero-variance in earlier epochs.</li>
                        <li><em><b>2)</b> Some zero-variance prompts can become effective again in future epochs:</em><br>
                        The <code>P(Current|Previous)</code> curve shows that approximately 20% of prompts previously labeled as zero-variance become effective prompts that contribute to training again. This suggests that, rather than statically pruning zero-variance prompts, it is beneficial to retain some degree of exploration to help retain potentially valuable prompts.</li>
                      </ul>
                  <h3 class="title is-5" > GRESO with Probabilistic Pre-rollout Prompt Filtering </h3>
                  <p>
                      Based on the above observations, as shown in the above Figure (b),
                      we propose a probabilistic filtering strategy based on reward dynamics observed during training. Rather than deterministically discarding prompts that previously yielded identical (i.e., zero-variance) rewards, we assign each prompt a probability of being filteredâ€”this probability increases with the number of recent consecutive rollouts where the prompt showed zero-variance. Concretely, for each prompt, we track how many times in a row it has produced zero-variance responses in the most recent epochs. The more consecutive times this occurs, the more likely the prompt will be skipped in the current rollout. However, we always retain a minimum exploration probability, ensuring that even frequently zero-variance prompts have a small chance of being re-sampled. This approach skips uninformative prompts to exploit training efficiency, while occasionally revisiting them to maintain exploration.
                  </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- Method-->
<!-- Results Section -->
<section class="section hero is-light" style="background-color: #FFFFFC !important;">
<!-- <section class="section hero is-light"> -->
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3" style="text-align: center;"><img src="static/images/exp.jpg" style="height: 35px; display: inline; vertical-align: -5px;"/> Comparable Performance with Fewer Rollouts</h2>
              <!-- Rollout -->
              <div class="content has-text-justified">
                <!-- <br> -->
                <!-- Up to 2.4$\times$ wall-clock time speed-up in rollout and  2.0$\times$ speed-up in training. -->
                <h3 class="title is-5" > No performance drop with up to 3.35x fewer rollouts </h3>
                <div class="figure">
                  <img src="static/images/table-rollout-comparison.jpg" alt="table-rollout-comparison" height="350" />
                </div>
                <p>
                    To verify the effectiveness of GRESO, we present a comprehensive evaluation of GRESO and Dynamic Sampling (DS), which filters out zero-variance examples and resamples to fill the batch with effective data, across six math reasoning benchmarks, using three different model settings in the above table.
                    The models are trained on either the DAPO + MATH dataset (DM) or the Open R1 subset (OR1).
                    We report both the performance and the number of rollouts from the checkpoint that achieves the best average performance across six benchmarks.
                    Across all training settings, GRESO achieves comparable accuracy as DS, while significantly reducing the number of rollout samplesâ€”achieving up to <b>3.35Ã— fewer rollouts</b>.
                    For example, on Qwen2.5-Math-7B trained on the DM dataset, GRESO achieves a comparable average accuracy to DS (57.5% vs. 57.8%), while reducing the number of rollouts from 13.1M to 6.3M.
                    These results demonstrate that GRESO maintains performance while substantially lowering the cost on rollouts.
                    Similar improvements are observed across other evaluation settings.
                </p>
              </div>

              <!-- Rollout end -->

              <!-- Wall-clock time -->
              <div class="content has-text-justified">
                <h3 class="title is-5" > Up to 2.0x wall-clock time speed-up in training </h3>
                <div style="display: flex; align-items: top; gap: 10px;">
                  <div style="flex: 1;">
                      <p>
                          To better understand the efficiency of our proposed methods, we report the detailed end-to-end training time breakdown for different stages: rollout, actor model update, and other overheads (e.g., reference model and advantage calculation).
                          Qwen2.5-Math-1.5B is trained on 4Ã—H100 GPUs, while the other two models are trained on 8Ã—H100 GPUs.
                          The right table compares the training time breakdown between GRESO and Dynamic Sampling for models trained on the DAPO + MATH dataset.
                          For all three models, GRESO significantly reduces rollout timeâ€”achieving up to <b>2.4Ã— speedup</b> in rollout and <b>2.0Ã— speedup</b> in total training time compared to DS.
                          For instance, on Qwen2.5-Math-7B, GRESO reduces rollout time from 155.9 hours to 65.5 hours, cutting overall training time from 178.0 to 88.3 hours.
                          </p>
                  </div>
                  <div style="flex: 0 0 45%; max-width: 45%;">
                      <img src="static/images/table-wall-clock-time.jpg" alt="rollout-scaling" width=450 />
                  </div>
                </div>
              </div>
              <!-- Wall-clock time end -->

              <!-- Case study -->
              <div class="content has-text-justified">
                <!-- <br> -->
                <h3 class="title is-5" > Case Study: Selection Dynamics </h3>
                <div style="display: flex; align-items: top; gap: 10px;">
                  <div style="flex: 1;">
                      <p>
                         Selection Dynamics of different prompts in GRESO. Each row is a prompt, and each column is an epoch.

                         We present a case study illustrating how GRESO selects or skips prompts over training epochs. We observe that very easy prompts tend to remain easy throughout training; although frequently skipped, GRESO still occasionally selects them to ensure a minimal level of exploration. For prompts of moderate difficulty, as the model becomes stronger over time, these prompts gradually become easier and are increasingly skipped. In contrast, some hard prompts become solvable~(i.e., effective prompts) in later epochs or even easy prompts. However, certain hard prompts remain unsolved throughout training.
                      </p>
                  </div>
                  <div style="flex: 0 0 30%; max-width: 30%;">
                      <img src="static/images/case-study.jpg" alt="rollout-scaling" width=350 />
                  </div>
                </div>
              </div>
              <!-- Case study end -->
            </div>
        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{chen2025knowcoder,
  title={KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA},
  author={Chen, Zhuo and Wang, Fei and Li, Zixuan and Zhang, Zhao and Ding, Weiwei and Yang, Chuanguang and Xu, Yongjun and Jin, Xiaolong and Guo, Jiafeng},
  journal={arXiv preprint arXiv:2510.25101},
  year={2025},
  eprint={2510.25101},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
  </html>